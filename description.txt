Description
===========

This project aims to build a basic lane and object detection system using a mix
of classical computer vision and deep learning. I used the nuScenes camera
images as the input because they contain real road scenes from multiple angles.

For lane detection, I used simple computer vision steps like converting the
image to grayscale, applying a region of interest, running Canny edge
detection, and then using the Hough Transform to find straight lane lines. This
method is not perfect, but it works well enough to highlight the lane
boundaries.

For object detection, I trained a YOLOv8n model using auto-labeled images from
all six nuScenes camera views. The model can detect vehicles, pedestrians,
traffic lights, and other common road objects. YOLO runs on each frame and
draws green bounding boxes with labels and confidence values.

Both lane detection and object detection run together in the final inference
script. All frames from the different camera folders are processed one by one,
and the script creates a final MP4 video where you can see the detected lanes,
objects, and FPS information.

The goal of this project was to combine two important ADAS components—lane
detection and object detection—into one simple pipeline and show the results in
a clear video format.
